# Using-AWS-S3-and-MySQL-to-build-a-data-pipeline-and-perform-ETL
### Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.

## Steps  
1. Create a bucket in Amazon S3 and store data in 
2. Get access key from AWS IAM role
3. Use hadoop package to read S3 file  
4. Set up sparkcontext configuration  
5. Do ETL


 
