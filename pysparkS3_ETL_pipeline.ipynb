{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd670dc",
      "metadata": {
        "id": "0bd670dc"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38389a9b",
      "metadata": {
        "id": "38389a9b"
      },
      "outputs": [],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d5c1ac",
      "metadata": {
        "id": "d3d5c1ac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7cdcf8f",
      "metadata": {
        "id": "a7cdcf8f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e592a73",
      "metadata": {
        "id": "0e592a73"
      },
      "outputs": [],
      "source": [
        "# defeining the access keys\n",
        "access_id =  \"AKIAV3D3SBAUFT73WIXP\"\n",
        "access_key = \"etruab3nvd+t6icUSTBbhYuJ3ytcxVm3NVWaRjH8\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a99bd8",
      "metadata": {
        "id": "21a99bd8"
      },
      "source": [
        "## Setting up the configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f1601b",
      "metadata": {
        "id": "f7f1601b"
      },
      "outputs": [],
      "source": [
        "# sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6a366f",
      "metadata": {
        "id": "bc6a366f"
      },
      "outputs": [],
      "source": [
        "# To create SparkContext\n",
        "# from pyspark import SparkContext\n",
        "import pyspark\n",
        "\n",
        "sc=pyspark.SparkContext()\n",
        "\n",
        "hadoop_conf=sc._jsc.hadoopConfiguration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04ef774",
      "metadata": {
        "id": "b04ef774"
      },
      "outputs": [],
      "source": [
        "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
        "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", access_id)\n",
        "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", access_key)\n",
        "hadoop_conf.set(\"fs.s3.buffer.dir\", \"${hadoop.tmp.dir}/s3\")\n",
        "hadoop_conf.set(\"fs.s3a.buffer.dir\", \"${hadoop.tmp.dir}/s3a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc45bb6f",
      "metadata": {
        "id": "bc45bb6f"
      },
      "source": [
        "## Extract Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79cf1524",
      "metadata": {
        "id": "79cf1524"
      },
      "outputs": [],
      "source": [
        "df=sc.textFile(\"s3a://testbucketpyspark20220917/airlines1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffe6534",
      "metadata": {
        "id": "8ffe6534"
      },
      "outputs": [],
      "source": [
        "df.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399af6d7",
      "metadata": {
        "id": "399af6d7"
      },
      "source": [
        "## Transform Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4817ee48",
      "metadata": {
        "id": "4817ee48"
      },
      "outputs": [],
      "source": [
        "Ardd = df.map(lambda x: x.replace(', ',' ').split(','))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c143fbf",
      "metadata": {
        "id": "5c143fbf"
      },
      "outputs": [],
      "source": [
        "AirlineRdd=Ardd.map(lambda x: Row(Year=x[1],FlightDate=x[6]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8321bc96",
      "metadata": {
        "id": "8321bc96"
      },
      "source": [
        "### Converting RDD to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafa83a7",
      "metadata": {
        "id": "fafa83a7"
      },
      "outputs": [],
      "source": [
        "from pyspark import SQLContext\n",
        "from pyspark.sql import Row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff457ead",
      "metadata": {
        "id": "ff457ead"
      },
      "outputs": [],
      "source": [
        "# setting up the sql context\n",
        "sqlContex = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2fb7ea",
      "metadata": {
        "id": "1c2fb7ea"
      },
      "outputs": [],
      "source": [
        "AirlineDf = sqlContex.createDataFrame(AirlineRdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63004084",
      "metadata": {
        "id": "63004084"
      },
      "outputs": [],
      "source": [
        "# seleting the years\n",
        "AirlineDf.select('Year','FlightDate').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531fc975",
      "metadata": {
        "id": "531fc975"
      },
      "outputs": [],
      "source": [
        "# How many flights are running each year\n",
        "AirlineDf.select('Year').groupBy(\"Year\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2404127e",
      "metadata": {
        "id": "2404127e"
      },
      "outputs": [],
      "source": [
        "#convert dataframe into Rdds\n",
        "rdds = AirlineDf.rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00640bf",
      "metadata": {
        "id": "f00640bf"
      },
      "outputs": [],
      "source": [
        "rdds.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34cfc8a5",
      "metadata": {
        "id": "34cfc8a5"
      },
      "source": [
        "## Loading Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305f5aad",
      "metadata": {
        "id": "305f5aad"
      },
      "outputs": [],
      "source": [
        "rdds.repartition(1).saveAsTextFile(\"s3a://testbucketpyspark20220917/report\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}